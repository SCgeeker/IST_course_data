---
title: "2-2. Bayesian Statistics"
author: "Sau-Chin Chen"
date: "2016年10月18日"
output: html_document
---
*Most information in this exercise comes Simon Jackman’s book ‘ Bayesian Analysis for the Social Sciences’, Alex Etz’s blog post on [updating priors via the likelihood](https://alexanderetz.com/2015/07/25/understanding-bayes-updating-priors-via-the-likelihood/), and Jim Granges’ blog post on [(pesky?) priors](https://jimgrange.wordpress.com/2016/01/18/pesky-priors/).* 

When we do research, we often start with a prior belief that a hypothesis is true. Then, 
we collect data, and we use this data to update our belief that a theory is true. Bayesian statistics allows you to update prior beliefs into posterior probabilities in a logically consistent manner. Before we have collected data, the **prior odds** of Hypothesis 1 (H1) over the null-hypothesis (H0) are P(H1)/P(H0), After we have collected data, we have the **posterior odds** P(H1|D)/P(H0|D), which you can read as the probability of H1, given the data, divided by the probability of H0, given the data. There are different approaches to Bayesian statistics. In this assignment we will first discuss Bayes factors, and then Bayesian estimation. 

### Bayes factor
The ratio of the prior odds and the posterior odds is the **Bayes factor**. In other words, the Bayes factor represents how much we have updated our beliefs, based on observing the data. We can express Bayes factors to indicate how much more likely H1 is given the data compared to H0 (often indicated by $B_{10}$ ) or as how much more likely H0 is compared to H1 ($B_{01}$), and $B_{10}$ = 1/$B_{01}$. Similar to likelihoods (and formally called a **marginal likelihood**, or a weighted average likelihood ratio), a Bayes factor of 1 did not change our beliefs (the posterior odds are exactly the same as our prior odds). A very large Bayes factor of H1 over H0 has increased our belief in H1, and a Bayes Factor close to 0 has increased our belief in H0. The contribution of the Bayes Factor and the prior in calculating the posterior odds is clear in the following formula: 
$$ \frac{P(H1|D)}{P(H0|D)} = \frac{P(D|H1)}{P(D|H0)} \times \frac{P(H1)}{P(H0)} $$
*Posterior Probability = Likelihood Ratio $\times$ Prior Probability* 

A Bayesian analysis of data requires specifying the prior. Here, we will continue our example based on a binomial probability, such as a coin flip. In the likelihood example, we compared two point hypotheses (e.g., $\theta$ = 0.5 vs. $\theta$ = 0.8). In Bayesian statistics, parameters are considered to be random variables, and the uncertainty or degree of belief with respect to the parameters is quantified by **probability distributions**. 
 
A binomial probability lies between 0 and 1. You could draw any probability density you want over 0 and 1, and turn it into a prior, but for good reasons (simplicity, mostly) a beta-prior is often used for binomial probabilities. The shape of the beta-prior depends on two parameters, $\alpha$ and $\beta$. Note that these are the same Greek letters as used for the Type 1 error rate and Type 2 error rate, but that is purely coincidental! The $\alpha$ and $\beta$ in binomial probabilities are unrelated to error rates, and the use of the same letters is mainly due to a lack of creativity among statisticians and the limited choice the alphabet gives us. It also does not help that $\beta$ is one of the parameters of the Beta distribution. Try to keep these different Beta’s apart! The probability density function is:

$$ \int (\chi, \alpha, \beta) = \frac{1}{B(\alpha,\beta)} \chi^{\alpha - 1} (1 - \chi)^{\beta - 1}$$

where $B(\alpha,\beta)$ is the beta function. Understanding the mathematical basis of this function is beyond the scope of this assignment, but you can read more on [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution) or in [John Kruschke’s book](https://sites.google.com/site/doingbayesiandataanalysis/) if interested. The beta-prior for a variety of values for $\alpha$ and $\beta$ can be seen in the figure below. 

```{r dbeta, echo=FALSE, message=FALSE, warning=FALSE}
theta = seq(0,1, len = 100)
par(mfrow=c(2,2))
plot(dbeta(theta, 1, 1) ~ theta, main = "α = 1, β = 1; Newborn's prior", ylab = "Density", xlab = expression(theta), type = 'l')
plot(dbeta(theta, 1, 1/2) ~ theta, main = "α = 1, β = 1/2; Believe always heads", ylab = "Density", xlab = expression(theta), type = 'l')
plot(dbeta(theta, 4, 4) ~ theta, main = "α = 4, β = 4; slightly believe fair", ylab = "Density; extremely believe fair", xlab = expression(theta), type = 'l')
plot(dbeta(theta, 100, 100) ~ theta, main = "α = 100, β = 100", ylab = "Density", xlab = expression(theta), type = 'l')
par(las = 0)
```

Let’s assume the newborn baby, the true believer, the slightly skeptic and the extreme skeptic all buy the coin, flip it n = 20 times, and observe x = 10 heads. This outcome can be plotted as a binomial distribution with 10 heads out of 20 trials, or as a Beta(11, 11) distribution. 

The newborn baby had a prior Beta distribution with α = 1 and β = 1, which equals a binomial likelihood distribution for 0 heads out of 0 trials. The posterior is a Beta distribution with Beta($\alpha^*, \beta^*$), where: 
 
$$ \alpha^* = \alpha + x = 1 + 10 = 11 $$
$$ \beta^* = \beta + n – x = 1 + 20 – 10 = 11 $$

Or calculating these values more directly from the α and β of the prior and likelihood: 
$$\alpha^* = \alpha_{prior} + \alpha_{likelihood} – 1 = 1 + 11 - 1= 11 $$
$$\beta^* = \beta_{prior} + \beta_{likelihood} – 1 = 1 + 11 - 1= 11 $$

Thus, the posterior distribution for the newborn is a Beta(11,11) distribution. This equals a binomial likelihood function for 10 heads out of 20 trials, or Beta(11,11) distribution. In other words, the posterior distribution is identical to the likelihood function when a uniform prior is used. 

Q1: The true believer had a prior of Beta(1,0.5). After observing 10 heads out of 20 coin flips, what is the posterior distribution, given that $\alpha^* = \alpha + x$ and $\beta^* = \beta + n – x$? 

Beta(1, 0.5) + Beta(10, 20 - 10) = **Beta(11, 10.5)**

Q2: The strong skeptic had a prior of Beta(100,100). After observing 50 heads out of 100 coin flips, what is the posterior distribution, given that $\alpha^* = \alpha + x$ and $\beta^* = \beta + n – x$? 

Beta(100,100) + Beta(50, 100 - 50) = **Beta(150, 150)**

Take a look at the top left graph below. Given 10 heads out of 20 coin flips, we see the prior distribution of the newborn (the horizontal grey line), the likelihood (the blue dotted line) and the posterior (the black line). 

For the true believer the posterior distribution is not centered on the maximum likelihood of the observed data, but just a bit in the direction of the prior. The slightly skeptic and the strong skeptic end up with a much stronger belief in a fair coin after observing the data, but mainly because they already had a stronger prior that the coin was fair. 


### Updating our belief 
Now that we have a distribution for the prior, and a distribution for the posterior, we can see in the graphs below for which values of $\theta$ our belief has increased. Everywhere where the black line (of the posterior) is higher than the grey line (of the prior) our belief in that $\theta$ has increased. 
 
The Bayes Factor is used to quantify this increase in relative evidence. Let’s calculate the Bayes Factor for the hypothesis that the coin is fair for the newborn. The Bayes Factor is simply the value of the posterior distribution at $\theta$ = 0.5, divided by the value of the prior distribution at $\theta$ = 0.5: 

BF10 = Beta($\theta$ = 0.5, 11, 11)/Beta($\theta$ = 0.5, 1, 1) = 3.70/1 = 3.70 

You can check this in an online Bayes Factor calculator by Jeff Rouder and Richard Morey: [http://pcl.missouri.edu/bf-binomial](http://pcl.missouri.edu/bf-binomial). At successes, fill in 10, at trials, fill in 20. We want to calculate the Bayes Factor for the point null value of $\theta$ = 0.5, so fill in 0.5. The $\alpha$ and $\beta$ for the prior are both 1, given the newborns prior of Beta(1,1). Clicking ‘submit query’ will give you the Bayes Factor of 3.70. 

Open the R script [BinomialBayesFactor.R](2.2-BinomialBayesFactor.R). This script requires 5 input parameters (identical to the Bayes Factor calculator website used above). These are the hypothesis you want to examine (e.g., when evaluating whether a coin is fair, $\theta$ = 0.5), the total number of trials (e.g., 20 flips), the number of successes (e.g., 10 heads), and the α and $\beta$ values for the Beta distribution for the prior (e.g., $\alpha$ = 1 and $\beta$ = 1 for a uniform prior). Run the script. It will calculate the Bayes Factor, and plot the prior (grey), likelihood (dashed blue) and posterior (black). For the example of 20 flips, 20 heads, and the newborn prior, the plot looks like this: 
```{r BBF01, echo=FALSE, message=FALSE, warning=FALSE}
H0<-0.5 #Set the point null hypothesis you want to calculate the Bayes Factor for
n<-20 #set total trials
x<-10 #set successes
aprior<-1 #Set the alpha for the Beta distribution for the prior
bprior<-1 #Set the beta for the Beta distribution for the prior

alikelihood<-x+1 #Calculate the alpha for the Beta distribution for the likelihood
blikelihood<-n-x+1 #Calculate the beta for the Beta distribution for the likelihood
aposterior<-aprior+alikelihood-1 #Calculate the alpha for the Beta distribution for the posterior
bposterior<-bprior+blikelihood-1 #Calculate the beta for the Beta distribution for the posterior

theta<-seq(0,1,0.001) #create theta range from 0 to 1
#png(file="PriorLikelihoodPosterior.png",width=3000,height=3000, res = 500)
prior <- dbeta(theta, aprior, bprior)
likelihood <- dbeta(theta, alikelihood, blikelihood)
posterior <- dbeta(theta, aposterior, bposterior)
plot(theta, posterior, ylim=c(0, 15), type = "l", lwd = 3, xlab = bquote(theta), ylab = "Density", las = 1)
lines(theta, prior, col="grey", lwd = 3)
lines(theta, likelihood, lty = 2, lwd = 3, col="dodgerblue")
BF10<-dbeta(H0, aposterior, bposterior)/dbeta(H0, aprior, bprior)
points(H0,dbeta(H0, aposterior, bposterior), pch = 19)
points(H0,dbeta(H0, aprior, bprior), pch = 19, col="grey")
segments(H0, dbeta(H0, aposterior, bposterior), H0, dbeta(H0, aprior, bprior), lty=2)
legend("topleft", c("Prior", "Posterior", "Likelihood"), col = c("grey", "black", "darkblue"), 
 lty = c(1,1,2), lwd = c(3,3,3), bty = "n", y.intersp = .55, x.intersp = .1, seg.len=.7)
title(paste('Bayes Factor:',round(BF10,digits=2)))
#dev.off()

#? Daniel Lakens, 2016. 
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/
```

We see that for the newborn, $\theta$ = 0.5 has become more probable, but so has $\theta$ = 0.4. 
 
Q3: Change the hypothesis in the first line from 0.5 to 0.675, and run the script. If you were testing the idea that this coin returns 67.5% heads, which statement is true? 
```{r Q3, echo=FALSE, message=FALSE, warning=FALSE}
H0<-0.675 #Set the point null hypothesis you want to calculate the Bayes Factor for
n<-20 #set total trials
x<-10 #set successes
aprior<-1 #Set the alpha for the Beta distribution for the prior
bprior<-1 #Set the beta for the Beta distribution for the prior

alikelihood<-x+1 #Calculate the alpha for the Beta distribution for the likelihood
blikelihood<-n-x+1 #Calculate the beta for the Beta distribution for the likelihood
aposterior<-aprior+alikelihood-1 #Calculate the alpha for the Beta distribution for the posterior
bposterior<-bprior+blikelihood-1 #Calculate the beta for the Beta distribution for the posterior

theta<-seq(0,1,0.001) #create theta range from 0 to 1
#png(file="PriorLikelihoodPosterior.png",width=3000,height=3000, res = 500)
prior <- dbeta(theta, aprior, bprior)
likelihood <- dbeta(theta, alikelihood, blikelihood)
posterior <- dbeta(theta, aposterior, bposterior)
plot(theta, posterior, ylim=c(0, 15), type = "l", lwd = 3, xlab = bquote(theta), ylab = "Density", las = 1)
lines(theta, prior, col="grey", lwd = 3)
lines(theta, likelihood, lty = 2, lwd = 3, col="dodgerblue")
BF10<-dbeta(H0, aposterior, bposterior)/dbeta(H0, aprior, bprior)
points(H0,dbeta(H0, aposterior, bposterior), pch = 19)
points(H0,dbeta(H0, aprior, bprior), pch = 19, col="grey")
segments(H0, dbeta(H0, aposterior, bposterior), H0, dbeta(H0, aprior, bprior), lty=2)
legend("topleft", c("Prior", "Posterior", "Likelihood"), col = c("grey", "black", "darkblue"), 
 lty = c(1,1,2), lwd = c(3,3,3), bty = "n", y.intersp = .55, x.intersp = .1, seg.len=.7)
title(paste('Bayes Factor:',round(BF10,digits=2)))
```

My belief in this hypothesis, given the data, would have stayed the same. 

Q4: Change the hypothesis in the first line back to 0.5. Let’s look at the increase in the belief of the hypothesis $\theta$ = 0.5 for the strong skeptic after 10 heads out of 20 coin flips. Change the $\alpha$ for the prior in line 4 to 100 and the $\beta$ for the prior in line 5 to 100. Run the script. Compare the Figure from R to the increase in belief for the newborn (in the plot on the previous page). Which statement is true? 
```{r Q4, echo=FALSE, message=FALSE, warning=FALSE}
0<-0.5 #Set the point null hypothesis you want to calculate the Bayes Factor for
n<-20 #set total trials
x<-10 #set successes
aprior<-100 #Set the alpha for the Beta distribution for the prior
bprior<-100 #Set the beta for the Beta distribution for the prior

alikelihood<-x+1 #Calculate the alpha for the Beta distribution for the likelihood
blikelihood<-n-x+1 #Calculate the beta for the Beta distribution for the likelihood
aposterior<-aprior+alikelihood-1 #Calculate the alpha for the Beta distribution for the posterior
bposterior<-bprior+blikelihood-1 #Calculate the beta for the Beta distribution for the posterior

theta<-seq(0,1,0.001) #create theta range from 0 to 1
#png(file="PriorLikelihoodPosterior.png",width=3000,height=3000, res = 500)
prior <- dbeta(theta, aprior, bprior)
likelihood <- dbeta(theta, alikelihood, blikelihood)
posterior <- dbeta(theta, aposterior, bposterior)
plot(theta, posterior, ylim=c(0, 15), type = "l", lwd = 3, xlab = bquote(theta), ylab = "Density", las = 1)
lines(theta, prior, col="grey", lwd = 3)
lines(theta, likelihood, lty = 2, lwd = 3, col="dodgerblue")
BF10<-dbeta(H0, aposterior, bposterior)/dbeta(H0, aprior, bprior)
points(H0,dbeta(H0, aposterior, bposterior), pch = 19)
points(H0,dbeta(H0, aprior, bprior), pch = 19, col="grey")
segments(H0, dbeta(H0, aposterior, bposterior), H0, dbeta(H0, aprior, bprior), lty=2)
legend("topleft", c("Prior", "Posterior", "Likelihood"), col = c("grey", "black", "darkblue"), 
 lty = c(1,1,2), lwd = c(3,3,3), bty = "n", y.intersp = .55, x.intersp = .1, seg.len=.7)
title(paste('Bayes Factor:',round(BF10,digits=2)))
```

The belief in the hypothesis that $\theta$ = 0.5, given the data, has **increased** for the strong skeptic,  but **not** as much as it has for the newborn. 
$\theta$ has been 0.5 from newborn to strong skeptic. 


Let’s assume the strong skeptic, who believes the coin is fair with a prior of Beta(100, 100), buys the coin and flips it 100 times. Surprisingly, the coin comes up heads 90 out of 100 flips. We can re-run the script, changing n in line 2 to 100, x in line 3 to 90, and setting α to 100 and β to 100 in lines 4 and 5. Running the script gives the following graph: 

```{r BBF02, echo=FALSE, message=FALSE, warning=FALSE}
H0<-0.5 #Set the point null hypothesis you want to calculate the Bayes Factor for
n<-100 #set total trials
x<-90 #set successes
aprior<-100 #Set the alpha for the Beta distribution for the prior
bprior<-100 #Set the beta for the Beta distribution for the prior

alikelihood<-x+1 #Calculate the alpha for the Beta distribution for the likelihood
blikelihood<-n-x+1 #Calculate the beta for the Beta distribution for the likelihood
aposterior<-aprior+alikelihood-1 #Calculate the alpha for the Beta distribution for the posterior
bposterior<-bprior+blikelihood-1 #Calculate the beta for the Beta distribution for the posterior

theta<-seq(0,1,0.001) #create theta range from 0 to 1
#png(file="PriorLikelihoodPosterior.png",width=3000,height=3000, res = 500)
prior <- dbeta(theta, aprior, bprior)
likelihood <- dbeta(theta, alikelihood, blikelihood)
posterior <- dbeta(theta, aposterior, bposterior)
plot(theta, posterior, ylim=c(0, 15), type = "l", lwd = 3, xlab = bquote(theta), ylab = "Density", las = 1)
lines(theta, prior, col="grey", lwd = 3)
lines(theta, likelihood, lty = 2, lwd = 3, col="dodgerblue")
BF10<-dbeta(H0, aposterior, bposterior)/dbeta(H0, aprior, bprior)
points(H0,dbeta(H0, aposterior, bposterior), pch = 19)
points(H0,dbeta(H0, aprior, bprior), pch = 19, col="grey")
segments(H0, dbeta(H0, aposterior, bposterior), H0, dbeta(H0, aprior, bprior), lty=2)
legend("topleft", c("Prior", "Posterior", "Likelihood"), col = c("grey", "black", "darkblue"), 
 lty = c(1,1,2), lwd = c(3,3,3), bty = "n", y.intersp = .55, x.intersp = .1, seg.len=.7)
title(paste('Bayes Factor:',round(BF10,digits=2)))
#dev.off()

#? Daniel Lakens, 2016. 
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/
```

We see the grey prior distribution, the dashed blue likelihood based on the data, and the posterior distribution in black. The Bayes Factor of 0 represents the substantial drop in belief that the coin is fair – indeed, this now seems an untenable hypothesis, even for the strong skeptic. It shows how data can update your belief. Where a newborn would now completely believe that the true $\theta$ for the coin is somewhere around 0.9, the strong skeptic has more reason to believe the $\theta$ is around 0.65, due to the strong prior conviction that the coin is fair. Given enough data, even this strong skeptic will become convinced that the coin will return heads most of the time as well. 

We can now also see the difference between a **likelihood inference approach**, and a **Bayesian inference approach**. In likelihood inference, you can compare different values of $\theta$ for the same likelihood curve (e.g., $\theta$ = 0.5 vs $\theta$ = 0.8) and calculate the likelihood ratio. In Bayesian inference, you can compare the difference between the prior and the posterior for the same value of θ, and calculate the Bayes Factor. 

If you have never seen Bayes Factors before, you might find it difficult to interpret the numbers. As with any guideline (e.g., interpreting effect sizes as small, medium, and large) there is criticism on the use of benchmarks. On the other hand, you have to start somewhere in getting a feel for what Bayes Factors mean. A Bayes factor between 1 and 3 is considered *not worth more than a bare mention*, larger than 3 (or smaller than 1/3) is considered *substantial*, and larger than 10 (or smaller than 1/10) is considered *strong*. These labels refer to the increase in how much you believe a specific hypothesis, not in the posterior belief in that hypothesis. If you think extra-sensory perception is extremely implausible, a single study with a BF = 14 will increase your belief, but you will now think extra-sensory perception is pretty much extremely implausible. 

### Bayesian Estimation 

The posterior distribution summarizes our belief about the expected number of heads when flipping a coin after seeing the data, by averaging over our prior beliefs and the data (or the likelihood). The mean of a Beta distribution can be calculated by $\frac{\alpha}{\alpha + \beta}$. We can thus easily calculate the mean of a posterior distribution, which is the expected value based on our prior beliefs and the data. 

We can also calculate a **credible interval** around the mean, which is a Bayesian version of a confidence interval with a slightly different interpretation. Instead of the Frequentist interpretation where a parameter has one (unknown) true value, the Bayesian approach considers the data fixed, but allow the parameter to vary. In Bayesian approaches, probability distributions represent our degree of belief. When calculating a credible interval, one is saying *I believe it is 95% probable (given my prior and the data) that the true parameter falls within this credible interval*. A 95% credible interval is simply the area of the posterior distribution between the 0.025 and 0.975 quantiles. 

*A credible interval and a confidence interval are the same, when a uniform prior (e.g., Beta(1,1)) is used. In this case, credible interval is numerically identical to the confidence interval.* **Only the interpretation differs.** Whenever an informed prior is used, the credible interval and confidence interval differ. If the chosen prior is not representative of the truth, the credible interval will not be representative of the truth, but it is always a correct formalization of your beliefs. **For a single confidence interval, the probability that it contains the true population parameter is either 0 or 1**. Only in the long run will 95% of confidence intervals contain the true population parameter. These are important differences between Bayesian credible intervals and Frequentist confidence intervals to keep in mind. 

Open the [BinomialPosteriorMean.R](2.2-BinomialPosteriorMean.R) script and run it. The script will plot the mean for the posterior when 10 heads out of 20 coin flips are observed, given a uniform prior. The script will also use the ‘binom’ package to calculate the posterior mean, credible interval, and highest density interval (HDI). The highest density interval is an alternative to the credible interval that works better when the posterior beta distribution is skewed (and is identical when the posterior distribution is symmetrical. We won’t go into the calculations of the HDI here.  

```{r BPM, echo=FALSE, message=FALSE, warning=FALSE}
n<-20 #set total trials
x<-10 #set successes
aprior<-1 #Set the alpha for the Beta distribution for the prior
bprior<-1 #Set the beta for the Beta distribution for the prior

ymax<-10 #set max y-axis

alikelihood<-x+1 #Calculate the alpha for the Beta distribution for the likelihood
blikelihood<-n-x+1 #Calculate the beta for the Beta distribution for the likelihood
aposterior<-aprior+alikelihood-1 #Calculate the alpha for the Beta distribution for the posterior
bposterior<-bprior+blikelihood-1 #Calculate the beta for the Beta distribution for the posterior

theta<-seq(0,1,0.001) #create theta range from 0 to 1
#png(file="BinomialPosteriorMean.png",width=4000,height=4000, res = 500)
prior <- dbeta(theta, aprior, bprior) #deterine prior distribution
likelihood <- dbeta(theta, alikelihood, blikelihood) #determine likelihood distribution
posterior <- dbeta(theta, aposterior, bposterior) #determine posterior distribution
plot(theta, posterior, ylim=c(0, ymax), type = "l", lwd = 3, xlab = bquote(theta), ylab = "Density", las = 1) #draw posterior distribution
lines(theta, prior, col="grey", lwd = 3) #draw prior distribution
lines(theta, likelihood, lty = 2, lwd = 3, col="dodgerblue") #draw likelihood distribution
LL<-qbeta(.025,aposterior, bposterior) #calculate lower limit credible interval
UL<-qbeta(.975,aposterior, bposterior) #calculate upper limit credible interval
abline(v = aposterior/(aposterior+bposterior)) #draw line mean
abline(v = LL, col="grey",lty=3) #draw line lower limit
abline(v = UL, col="grey",lty=3) #draw line upper limit
polygon(c(theta[theta<LL],rev(theta[theta<LL])),c(posterior[theta<LL], rep(0,sum(theta<LL))),col="lightgrey",border=NA)
polygon(c(theta[theta>UL],rev(theta[theta>UL])),c(posterior[theta>UL], rep(0,sum(theta>UL))),col="lightgrey",border=NA)
legend("topleft", c("Prior", "Posterior", "Likelihood"), col = c("grey", "black", "darkblue"), 
 lty = c(1,1,2), lwd = c(3,3,3), bty = "n", y.intersp = .55, x.intersp = .1, seg.len=.7)
title(paste('Mean posterior:',round((aposterior/(aposterior+bposterior)),digits=5),", 95% Credible Interval:",round(LL,digits=2),";",round(UL,digits=2)))
#dev.off()

if(!require(binom)){install.packages('binom')}
library(binom)
binom.bayes(x, n, type = "central", prior.shape1 = aprior, prior.shape2 = bprior)
binom.bayes(x, n, type = "highest", prior.shape1 = aprior, prior.shape2 = bprior)

#? Daniel Lakens, 2016. 
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/
```

The plot above is the output of running the script. The posterior mean is identical to the Frequentist mean, but this is only the case *when the mean of the prior equals the mean of the likelihood*.  

Q5: Assume the outcome of 20 coin flips had been 18 heads. Change x to 18 in line 2 and run the script. Remember that the mean of the prior Beta(1,1) distribution is $frac{\alpha}{\alpha + \beta}$, or 1/(1+1) = 0.5. The Frequentist mean is simply x/n, or 18/20=0.9. Which statement is true?

```{r Q5, echo=FALSE, message=FALSE, warning=FALSE}
n<-20 #set total trials
x<-18 #set successes
aprior<-1 #Set the alpha for the Beta distribution for the prior
bprior<-1 #Set the beta for the Beta distribution for the prior

ymax<-10 #set max y-axis

alikelihood<-x+1 #Calculate the alpha for the Beta distribution for the likelihood
blikelihood<-n-x+1 #Calculate the beta for the Beta distribution for the likelihood
aposterior<-aprior+alikelihood-1 #Calculate the alpha for the Beta distribution for the posterior
bposterior<-bprior+blikelihood-1 #Calculate the beta for the Beta distribution for the posterior

theta<-seq(0,1,0.001) #create theta range from 0 to 1
#png(file="BinomialPosteriorMean.png",width=4000,height=4000, res = 500)
prior <- dbeta(theta, aprior, bprior) #deterine prior distribution
likelihood <- dbeta(theta, alikelihood, blikelihood) #determine likelihood distribution
posterior <- dbeta(theta, aposterior, bposterior) #determine posterior distribution
plot(theta, posterior, ylim=c(0, ymax), type = "l", lwd = 3, xlab = bquote(theta), ylab = "Density", las = 1) #draw posterior distribution
lines(theta, prior, col="grey", lwd = 3) #draw prior distribution
lines(theta, likelihood, lty = 2, lwd = 3, col="dodgerblue") #draw likelihood distribution
LL<-qbeta(.025,aposterior, bposterior) #calculate lower limit credible interval
UL<-qbeta(.975,aposterior, bposterior) #calculate upper limit credible interval
abline(v = aposterior/(aposterior+bposterior)) #draw line mean
abline(v = LL, col="grey",lty=3) #draw line lower limit
abline(v = UL, col="grey",lty=3) #draw line upper limit
polygon(c(theta[theta<LL],rev(theta[theta<LL])),c(posterior[theta<LL], rep(0,sum(theta<LL))),col="lightgrey",border=NA)
polygon(c(theta[theta>UL],rev(theta[theta>UL])),c(posterior[theta>UL], rep(0,sum(theta>UL))),col="lightgrey",border=NA)
legend("topleft", c("Prior", "Posterior", "Likelihood"), col = c("grey", "black", "darkblue"), 
 lty = c(1,1,2), lwd = c(3,3,3), bty = "n", y.intersp = .55, x.intersp = .1, seg.len=.7)
title(paste('Mean posterior:',round((aposterior/(aposterior+bposterior)),digits=5),", 95% Credible Interval:",round(LL,digits=2),";",round(UL,digits=2)))
#dev.off()

#if(!require(binom)){install.packages('binom')}
#library(binom)
#binom.bayes(x, n, type = "central", prior.shape1 = aprior, prior.shape2 = bprior)
#binom.bayes(x, n, type = "highest", prior.shape1 = aprior, prior.shape2 = bprior)

#? Daniel Lakens, 2016. 
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/
```
The frequentist mean is **high** than the mean of the posterior, because the mean of the posterior is **futher from** to the mean of the prior distribution. 
**frequentist mean is 1**


Q6: What is, today, your best estimate of the probability that the sun rises every day? Assume you were born with an uniform Beta(1,1) prior. The sun can either rise, or it does not. Assume you have seen the sun every day since you were born, which means there has been a continuous string of successes for every day you have been alive. It is ok to estimate the days you have been alive by just multiplying your age by 365 days. What is your best estimate of the probability that the sun will rise?

```{r Q6, echo=FALSE, message=FALSE, warning=FALSE}
n<-40*365 #set total trials
x<-40*365 #set successes
aprior<-1 #Set the alpha for the Beta distribution for the prior
bprior<-1 #Set the beta for the Beta distribution for the prior

ymax<-10 #set max y-axis

alikelihood<-x+1 #Calculate the alpha for the Beta distribution for the likelihood
blikelihood<-n-x+1 #Calculate the beta for the Beta distribution for the likelihood
aposterior<-aprior+alikelihood-1 #Calculate the alpha for the Beta distribution for the posterior
bposterior<-bprior+blikelihood-1 #Calculate the beta for the Beta distribution for the posterior

theta<-seq(0,1,0.001) #create theta range from 0 to 1
#png(file="BinomialPosteriorMean.png",width=4000,height=4000, res = 500)
prior <- dbeta(theta, aprior, bprior) #deterine prior distribution
likelihood <- dbeta(theta, alikelihood, blikelihood) #determine likelihood distribution
posterior <- dbeta(theta, aposterior, bposterior) #determine posterior distribution
plot(theta, posterior, ylim=c(0, ymax), type = "l", lwd = 3, xlab = bquote(theta), ylab = "Density", las = 1) #draw posterior distribution
lines(theta, prior, col="grey", lwd = 3) #draw prior distribution
lines(theta, likelihood, lty = 2, lwd = 3, col="dodgerblue") #draw likelihood distribution
LL<-qbeta(.025,aposterior, bposterior) #calculate lower limit credible interval
UL<-qbeta(.975,aposterior, bposterior) #calculate upper limit credible interval
abline(v = aposterior/(aposterior+bposterior)) #draw line mean
abline(v = LL, col="grey",lty=3) #draw line lower limit
abline(v = UL, col="grey",lty=3) #draw line upper limit
polygon(c(theta[theta<LL],rev(theta[theta<LL])),c(posterior[theta<LL], rep(0,sum(theta<LL))),col="lightgrey",border=NA)
polygon(c(theta[theta>UL],rev(theta[theta>UL])),c(posterior[theta>UL], rep(0,sum(theta>UL))),col="lightgrey",border=NA)
legend("topleft", c("Prior", "Posterior", "Likelihood"), col = c("grey", "black", "darkblue"), 
 lty = c(1,1,2), lwd = c(3,3,3), bty = "n", y.intersp = .55, x.intersp = .1, seg.len=.7)
title(paste('Mean posterior:',round((aposterior/(aposterior+bposterior)),digits=5),", 95% Credible Interval:",round(LL,digits=2),";",round(UL,digits=2)))
#dev.off()

#if(!require(binom)){install.packages('binom')}
#library(binom)
#binom.bayes(x, n, type = "central", prior.shape1 = aprior, prior.shape2 = bprior)
#binom.bayes(x, n, type = "highest", prior.shape1 = aprior, prior.shape2 = bprior)

#? Daniel Lakens, 2016. 
# This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. https://creativecommons.org/licenses/by-nc-sa/4.0/
```

0.999

Q7: What would have been the best estimate from a Frequentist perspective?  

0

Q8:  Reflect  in  100  words  on  whether  you  believe  observing  only  confirming  evidence should ever lead to an estimate of 100%, or whether there should always be some small remaining doubt. 

> If I am Bayesian statistician, I will increase my belief evidence by evidence. If I am Frequentist, I will never obtain the negative evidence happened in the long run. The more important question is: may my mind have both Bayesian and Frequentist?

This exercise shows the essence of Bayesian inference, where we decide upon a prior distribution, collect data and calculate a marginal likelihood distribution, and use these distributions to calculate a posterior distribution. From this posterior distribution, we can estimate the mean of the posterior distribution and the 95% credible interval. For any specific  hypothesis,  we  can  calculate  the  relative  evidence  for  a  posterior  model, compared  to  a  prior  model,  through  the  Bayes  Factor.  Thus,  we  can  use  Bayesian statistics to quantify relative evidence, which can inform us how much we should believe, or update our beliefs, in theories. 
