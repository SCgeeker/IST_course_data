This week, we will dive into the topic of error control in much more detail. More specifically, we will look at how error rates can easily be inflated (but you typically don’t know by how much!). Often, it is possible to do all the analyses you want to do, while controlling your error rates. We will discuss how to do this in relation to ‘optional stopping’, which is performing a statistical test multiple times, as the data comes in, in assignment 3.2. In a Neyman-Pearson approach to using p-values, you should not only control the Type 1 error rate, but also the Type 2 error rate. The concept of statistical power (finding an effect, if it is really there) is explained in lecture 3.2. In assignment 3.2, you will see how low statistical power and inflated Type 1 error rates, together with publication bias, could lead to a scientific literature where most published findings are false. In lecture 3.3, you will learn the benefits of pre-registration to fix the Type 1 error rate in studies you perform. You will pre-register a small study in assignment 7.1. And check out the interview with Dan Simons on Registered Replication Reports to learn how pre-registration is used in psychology!
        
        Can’t get enough? Some suggestions for additional reading:
        
        Lecture 3.1

The quite recent, but already classic paper illustrating the problems of inflated Type 1 error rates is:
        
        Simmons, J. P., Nelson, L. D., & Simonsohn, U. (2011). False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant. Psychological Science, 22(11), 1359–1366. http://doi.org/10.1177/0956797611417632

A good discussion of Type 1 error control is provided by:
        
        Rutherford, A. (2011). ANOVA and ANCOVA: a GLM approach (2nd ed). Hoboken, N.J: Wiley. Sections 3.6-3.10

If you want to use sequential analyses and benefit from their efficiency, without inflating type 1 error rates, I’ve written an accessible introduction:
        
        Lakens, D. (2014). Performing high-powered studies efficiently with sequential analyses: Sequential analyses. European Journal of Social Psychology, 44(7), 701–710. http://doi.org/10.1002/ejsp.2023

A discussion of a way to directly control the false discovery rate can be found in:
        
        Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. Journal of the Royal Statistical Society. Series B (Methodological), 289–300.

Lecture 3.2

Cohen gives an accessible introduction to power, and why it is important, here:
        
        Cohen, J. (1992). Statistical power analysis. Current Directions in Psychological Science, 1(3), 98–101.

A good article on weighing how bad a Type 1 error or a Type 2 error is is provided by Fiedler and colleagues:
        
        Fiedler, K., Kutzner, F., & Krueger, J. I. (2012). The Long Way From -Error Control to Validity Proper: Problems With a Short-Sighted False-Positive Debate. Perspectives on Psychological Science, 7(6), 661–669. http://doi.org/10.1177/1745691612462587

We discuss the importance of increasing the informational value of studies (either by improving the accuracy of estimates, or power analyses, and why large enough sample sizes are important) here:
        
        Lakens, D., & Evers, E. R. K. (2014). Sailing From the Seas of Chaos Into the Corridor of Stability: Practical Recommendations to Increase the Informational Value of Studies. Perspectives on Psychological Science, 9(3), 278–292. http://doi.org/10.1177/1745691614528520

Assignment 3.2 is based on the two articles below:
        
        Ioannidis, J. P. A. (2005). Why Most Published Research Findings Are False. PLoS Medicine, 2(8), e124. http://doi.org/10.1371/journal.pmed.0020124

Wacholder, S., Chanock, S., Garcia-Closas, M., El ghormli, L., & Rothman, N. (2004). Assessing the Probability That a Positive Report is False: An Approach for Molecular Epidemiology Studies. JNCI Journal of the National Cancer Institute, 96(6), 434–442. http://doi.org/10.1093/jnci/djh075

Lecture 3.3

As solutions to the file-drawer problem, registered reports have been proposed. The three articles below introduce this novel format, and discuss how it can improve science.

Nosek, B. A., & Lakens, D. (2014). Registered reports: A method to increase the credibility of published results. Social Psychology, 45(3), 137–141. http://doi.org/10.1027/1864-9335/a000192

Chambers, C. D., Feredoes, E., Muthukumaraswamy, S. D., & Etchells, P. (2014). Instead of“ playing the game” it is time to change the rules: Registered Reports at AIMS Neuroscience and beyond. AIMS Neuroscience, 1(1), 4–17.

Simons, D. J., Holcombe, A. O., & Spellman, B. A. (2014). An introduction to registered replication reports at perspectives on psychological science. Perspectives on Psychological Science, 9(5), 552–555.
